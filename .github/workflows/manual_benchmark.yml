name: Run Benchmark

on:
  workflow_dispatch:

env:
  GLOBAL_BENCHMARK_JSON: '["synthetic_2C1D_1C"]'

permissions:
  contents: read
  id-token: write

jobs:
  prepare:
    runs-on: ubuntu-latest
    outputs:
      benchmarks_to_execute: ${{ steps.json.outputs.benchmarks_to_execute }}
    steps:
      - name: build matrix
        id: json
        run: |
          benchmarks_to_execute='{ "repository": ${{ env.GLOBAL_BENCHMARK_JSON }} }'
          echo 'benchmarks_to_execute=$benchmarks_to_execute' >> "$GITHUB_OUTPUT"

  add-runner:
    needs: prepare
    runs-on: ubuntu-latest
    env:
      benchmarks_to_execute: ${{ needs.prepare.outputs.benchmarks_to_execute }}
    steps:
      - name: Generate a token
        id: generate-token
        uses: actions/create-github-app-token@v1
        with:
          app-id: ${{ vars.APP_ID }}
          private-key: ${{ secrets.APP_PRIVATE_KEY }}
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          role-session-name: Github_Add_Runner
          aws-region: eu-central-1
      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2
      - name: Execute Lambda function
        run: |
          echo '${{ env.benchmarks_to_execute }}'
          number_of_tasks=$(echo '${{ env.benchmarks_to_execute }}'' | jq '. | length')
          echo "Number of tasks: $number_of_tasks"
          aws lambda invoke --function-name jit_runner_register_and_create_runner_container  --cli-binary-format raw-in-base64-out --payload '{"github_api_secret": "${{ steps.generate-token.outputs.token }}", "count_container":  $number_of_tasks, "container_compute": "M", "repository": "${{ github.repository }}" }'  response.json
          cat response.json
          if ! grep -q '"statusCode": 200' response.json; then
            echo "Lambda function failed. statusCode is not 200."
            exit 1
          fi

  benchmark-test:
    needs: [prepare, add-runner]
    runs-on: self-hosted
    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.prepare.outputs.benchmarks_to_execute) }}
    timeout-minutes: 1440
    env:
      BAYBE_BENCHMARKING_PERSISTENCE_PATH: ${{ secrets.TEST_RESULT_S3_BUCKET }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - uses: actions/setup-python@v5
        id: setup-python
        with:
          python-version: "3.10"
      - name: Benchmark
        run: |
          pip install '.[benchmarking]'
          python -m benchmarks "${{ matrix.benchmarks_to_execute }}"
